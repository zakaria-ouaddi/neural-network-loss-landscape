# ğŸ”ï¸ Gradient Descent Loss Landscape Visualizer

> Visualize how gradient descent navigates loss surfaces â€” from convex logistic regression to non-convex neural networks â€” with interactive 3D plots, animated trajectories, and curvature analysis.

![Loss Surface 3D](figures/logistic_3d.png)

## ğŸ¯ What This Project Does

This project provides a complete pipeline to:

1. **Train models** (logistic regression & two-layer MLP) on synthetic data
2. **Collect weight checkpoints** throughout training
3. **Project** the high-dimensional weight trajectory into 2D using PCA (with filter normalization) and Isomap
4. **Reconstruct the loss surface** over the projected plane
5. **Visualize** everything as:
   - ğŸ“Š Static 3D surface plots with gradient arrows
   - ğŸ¬ Animated GIFs showing GD step-by-step on a rotating surface
   - ğŸ–±ï¸ Interactive HTML plots (Plotly) â€” rotate, zoom, hover for epoch & loss info

## ğŸ“¸ Gallery

### Convex vs Non-Convex
| Logistic Regression (Convex) | MLP (Non-Convex) |
|:---:|:---:|
| ![Logistic](figures/logistic_3d.png) | ![MLP](figures/mlp_3d.png) |

### Multi-Seed Trajectories â€” Different Initializations, Different Minima
![Multi-seed](figures/mlp_multi_seed.png)

### Hessian Curvature Along the Path
![Curvature](figures/mlp_curvature.png)

### PCA vs Isomap Comparison
![PCA vs Isomap](figures/mlp_pca_vs_isomap.png)

### Optimizer Comparison (SGD vs Momentum vs Adam)
![Optimizers](figures/optimizer_comparison.png)

## ğŸš€ Quick Start

```bash
# Clone
git clone https://github.com/YOUR_USERNAME/gradient-descent-landscape-viz.git
cd gradient-descent-landscape-viz

# Setup
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Run everything
python main.py
```

All outputs (PNGs, animated GIFs, interactive HTMLs) are saved to `figures/`.

Open any `.html` file in your browser for interactive 3D exploration â€” no server needed.

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py               # End-to-end orchestrator â€” run this
â”œâ”€â”€ config.py             # All hyperparameters in one place
â”œâ”€â”€ data.py               # Synthetic data generation (make_classification)
â”œâ”€â”€ models.py             # LogisticRegression & MLP (PyTorch)
â”œâ”€â”€ training.py           # Training loop + multi-seed experiments
â”œâ”€â”€ projection.py         # PCA & Isomap dimensionality reduction
â”œâ”€â”€ filter_norm.py        # Per-layer filter normalization (Li et al. 2018)
â”œâ”€â”€ loss_surface.py       # 2D grid loss evaluation
â”œâ”€â”€ gradients.py          # Gradient projection onto PCA plane
â”œâ”€â”€ visualization.py      # Static matplotlib plots
â”œâ”€â”€ animations.py         # Animated GIF generation
â”œâ”€â”€ interactive_plots.py  # Interactive Plotly HTML generation
â”œâ”€â”€ hessian.py            # Hessian eigenvalue analysis (curvature)
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ what_where.md         # Task-to-code mapping
â””â”€â”€ figures/              # All generated outputs
```

## ğŸ”¬ Key Concepts

| Concept | Description |
|---------|-------------|
| **PCA Projection** | Projects weight trajectory from â„á´° â†’ â„Â² using the top-2 principal components centered at the final optimum Î¸* |
| **Filter Normalization** | Rescales PCA direction vectors per layer so â€–dâ‚—â€– = â€–Î¸*â‚—â€–, preventing large layers from dominating (Li et al., 2018) |
| **Loss Surface** | Evaluates L(Î¸* + Î±u + Î²v) over a 2D grid in PCA space |
| **Gradient Arrows** | Full-batch gradients projected onto the PCA plane, showing descent direction |
| **Hessian Curvature** | Top eigenvalue of the Hessian via power iteration â€” measures surface sharpness |
| **Isomap** | Non-linear dimensionality reduction for comparison with PCA |

## âš™ï¸ Configuration

All parameters are in [`config.py`](config.py). Key ones:

```python
n_samples = 1000        # dataset size
n_features = 10         # input dimensions
hidden_dim = 32         # MLP hidden layer size
epochs = 100            # training epochs
lr = 0.05               # learning rate
vis_resolution = 50     # loss surface grid resolution
seed_list = [0..9]      # seeds for multi-run experiments
```

## ğŸ“š References

- Li, H., Xu, Z., Taylor, G., Studer, C., & Goldstein, T. (2018). *Visualizing the Loss Landscape of Neural Nets.* NeurIPS.
- Goodfellow, I., Vinyals, O., & Saxe, A. (2015). *Qualitatively characterizing neural network optimization problems.* ICLR.

## ğŸ“„ License

MIT

---

*Built as part of the Advanced Machine Learning course at the University of Bremen.*
